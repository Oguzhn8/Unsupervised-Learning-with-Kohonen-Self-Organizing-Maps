# Unsupervised-Learning-with-Kohonen-Self-Organizing-Maps
Pioneered in 1982 by Finnish professor and researcher Dr. Teuvo Kohonen, a self-organising map is an unsupervised learning model, intended for applications in which maintaining a topology between input and output spaces is of importance. The notable characteristic of this algorithm is that the input vectors that are close — similar — in high dimensional space are also mapped to nearby nodes in the 2D space. It is in essence a method for dimensionality reduction, as it maps high-dimension inputs to a low (typically two) dimensional discretised representation and conserves the underlying structure of its input space. <br/>
A valuable detail is that the entire learning occurs without supervision i.e. the nodes are self-organising. They are also called feature maps, as they are essentially retraining the features of the input data, and simply grouping themselves according to the similarity between one another. This has a pragmatic value for visualising complex or large quantities of high dimensional data and representing the relationship between them into a low, typically two-dimensional, field to see if the given unlabelled data has any structure to it. The structure of the Kohonen network is as follows: 
![image](https://user-images.githubusercontent.com/78887209/216679098-09c5183b-ad7d-46bf-9e68-403962bd23f5.png)
A Self-Organizing Map (SOM) differs from typical ANNs both in its architecture and algorithmic properties. Firstly, its structure comprises of a single-layer linear 2D grid of neurons, instead of a series of layers. All the nodes on this grid are connected directly to the input vector, but not to one another, meaning the nodes do not know the values of their neighbours, and only update the weight of their connections as a function of the given inputs. The grid itself is the map that organises itself at each iteration as a function of the input of the input data. As such, after clustering, each node has its own (i,j) coordinate, which allows one to calculate the Euclidean distance between 2 nodes by means of the Pythagorean theorem.
## Attributes
Additionally, a Self-Organising Map adjusts its weights via competitive learning rather than error-correction learning. Because each node competes for the right to respond to the input, only one node is active at a time when the neural network is provided with the features of a particular instance of the input vector.The chosen node — the Best Matching Unit (BMU) — is selected according to the similarity, between the current input values and all the nodes in the grid. <br/>
The node with the least Euclidean difference between the input vector and all other nodes is selected, and its nearby nodes within a predetermined radius are also chosen. These nodes' positions are then gently modified to match the input vector.
The entire grid eventually matches the full input dataset after traversing over every node in the grid, with dissimilar nodes being separated and like nodes being gathered together toward a certain location. <br/>
![image](https://user-images.githubusercontent.com/78887209/216680393-de0990a8-592b-4b49-bff0-72c5808948e5.png)
## Algorithm 
1.Initialise each node’s weight w_ij to a random value <br/>
2.Select a random input vector x_k <br/>
3.Repeat point 4. and 5. for all nodes in the map: <br/>
4.Compute Euclidean distance between the input vector x(t) and the weight vector w_ij associated with the first node, where t, i, j = 0. <br/>
5.Track the node that produces the smallest distance t. <br/>
6.Find the overall Best Matching Unit (BMU), i.e. the node with the smallest distance from all calculated ones. <br/>
7.Determine topological neighbourhood βij(t) its radius σ(t) of BMU in the Kohonen Map <br/>
8.Repeat for all nodes in the BMU neighbourhood: Update the weight vector w_ij of the first node in the neighbourhood of the BMU by adding a fraction of the difference between the input vector x(t) and the weight w(t) of the neuron. <br/>
9.Repeat this whole iteration until reaching the chosen iteration limit t=n <br/>
Step 1 is the initialisation phase, while step 2–9 represent the training phase. <br/>
## Application 1
The first problem will be a simple problem to test if the network is working properly. It will create four different, three-dimensional sets of points, each with 150 points. An easy way to do this is to create the three-dimensional points of each cluster with Gaussian distributions with different mean and standard deviations for each component. A similar problem is discussed in S. Haykin's book “Neural Networks: A Comprehensive Foundation” between pages 483-485. Although the information about which points belong to which classes is known in the 600-element dataset, this class information will not be used while training the network, and the system with 1- and/or 2-dimensional neighborhood defined for the Kohonen network will be used. <br/>
The distribution of 4 different sets of points created for the problem can be seen below:
![image](https://user-images.githubusercontent.com/78887209/216683450-4cd4aa9d-7ec4-44df-b124-0c2d087c4554.png) <br/>
The number of neurons was chosen as 20 and the initial weight matrix was formed from randomly generated numbers between -1 and 1. Afterwards, an index_map function is defined that specifies the position of each neuron in the coordinate system, which can be used while training the network. Since the index_map function will be used in 2-dimensional neighborhoods, it is defined to be 2-dimensional. Out of 600 points, 450 are reserved for the training set and 150 for the test set. After defining the sigma function to be used to define the neighborhood, the Kohonen network has a network structure like “winner shares with neighbor neurons” in that iteration.
A function is defined that finds the neighborhood of the neuron that will win. The number of iterations was determined as 1000 and the learning rate was determined as 0.05. In order to find the winning neuron while training the network, the point in the data set and the norms of the neurons are assigned to a list, and the index with the smallest norm is the index of the winning neuron at that step. Then, the class information at that point is assigned to the neuron class and the neuron is assigned to the relevant class. By calling the neighborhood function, which was defined before, so that the close neighbors of the winning neuron are affected more and the distant neighbors less affected, it is ensured that the winning neuron also trains the neighboring neurons. Before the weights are updated, the existing weights are saved for use in the next weight update. Afterwards, the weights were updated in accordance with the Hebb Learning Rule. The stopping criterion was determined as "stop the self-regulation phase if the mean of the weight change was less than 0.01". After the training stops, it is aimed to increase the convergence of the neurons to the classes by updating the weights with 500x20 (number of neurons) steps so that the weights can represent the classes much better. “root mean squared error” and “mean absolute error” are used as error functions. The test set was tested with the weights obtained at the end of 10000 steps. As an error criterion, "the distance of the winning neuron to the point was considered as an error by finding the norms of the points in the test set and the weights obtained at the end of 10000 steps, respectively." With this method, it has been tried to determine how much a group is similar to each other. Since the weights from the training set are the weights that converge to the classes, the weights have not been updated in the defect-finding operations on the test set.
The test set mean absolute error was 0.84100 as the average of 20 tests. <br/>
The test set mean square root error was 0.98458 as the average of 20 trials. <br/>
## Application 2
In this problem, the Iris dataset will be handled as a clustering problem with the help of the Kohonen network designed in the first application.
Kohonen network consists of a single-layer linear 2-dimensional neuron grid instead of a series of layers. All nodes in the grid, namely neurons, are directly connected to the input vector, and the neurons update the weights of their neighbors as a function of the given inputs. The grid of neurons organizes and updates itself with each iteration. As training continues, neurons move closer to the classes they represent. In this way, the clusters are separated from each other and the classification process is done. Since there are slightly fewer points than in the first application, an equal number of records from the characteristics of each flower species were distributed to the training set and the test set in this problem. The number of neurons was chosen as 20 in this problem as well. Unlike the first implementation, the initial weights were created from random numbers between -2.5 and 2.5. Unlike the first question, the stopping criterion was determined as "stop the self-regulation phase if the mean of the weight change is less than 0.001". The mean square error value was found to be 2.88758 in 20 simulations.
## Further Readings
https://towardsdatascience.com/kohonen-self-organizing-maps-a29040d688da <br/>
https://researchleap.com/k-means-clustering/ <br/>
https://medium.com/neuronio/discovering-som-an-unsupervised-neural-network-12e787f38f9 <br/>
